Setup:

gcloud services enable \
  compute.googleapis.com \
  monitoring.googleapis.com \
  logging.googleapis.com \
  notebooks.googleapis.com \
  aiplatform.googleapis.com \
  artifactregistry.googleapis.com \
  container.googleapis.com

----------------------------------------------------------------------

Task 1: Create a Vertex AI Workbench instance

Navigate to Vertex AI > Workbench.

Configure the Instance:
    Name: Provide a name for your instance as cnn-challenge.
    Region: Set the region to us-west1
    Zone: Set the zone to us-west1-c

-----------------------------------------------------------------------

Task 2: Copy the notebook from a Cloud Storage bucket

In your notebook, click the terminal.

Copy the below path in terminal.

gcloud storage cp gs://qwiklabs-gcp-01-f3df77f6f1ae-labconfig-bucket/cnn_challenge_lab-v1.0.0.ipynb .


In ipynb file:

import os

! pip3 install --user --upgrade google-cloud-aiplatform
! pip3 install --user --upgrade google-cloud-storage
! pip3 install --user --upgrade google-cloud-logging
! pip3 install --user protobuf==3.20
! pip3 install --user --upgrade pillow

import os

if not os.getenv("IS_TESTING"):
    # Automatically restart kernel after installs
    import IPython

    app = IPython.Application.instance()
    app.kernel.do_shutdown(True)

    import os

# Retrieve and set PROJECT_ID environment variables.
# TODO: fill in PROJECT_ID.

if not os.getenv("IS_TESTING"):
    # Get your Google Cloud project ID from gcloud
    PROJECT_ID = ! gcloud config get project
    PROJECT_ID = PROJECT_ID[0]

PROJECT_ID

from datetime import datetime
TIMESTAMP = datetime.now().strftime("%Y%m%d%H%M%S")

REGION = " "  # @param {type:"string"}

# TODO: Create a globally unique Google Cloud Storage bucket name for artifact storage.
# HINT: Start the name with gs://
BUCKET_NAME = f"gs://{PROJECT_ID}"

! gsutil mb -l $REGION $BUCKET_NAME

-------------------------------------------------------------------------------------------------

Task 3. Create a training script

import os
import sys

from google.cloud import aiplatform
from google.cloud.aiplatform import gapic as aip

aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)


TRAIN_GPU, TRAIN_NGPU = (None, None)
DEPLOY_GPU, DEPLOY_NGPU = (None, None)


TRAIN_VERSION = "tf-cpu.2-8"
DEPLOY_VERSION = "tf2-cpu.2-8"

TRAIN_IMAGE = "us-docker.pkg.dev/vertex-ai/training/{}:latest".format(TRAIN_VERSION)
DEPLOY_IMAGE = "us-docker.pkg.dev/vertex-ai/prediction/{}:latest".format(DEPLOY_VERSION)

print("Training:", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)
print("Deployment:", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)


MACHINE_TYPE = "n1-standard"

VCPU = "4"
TRAIN_COMPUTE = MACHINE_TYPE + "-" + VCPU
print("Train machine type", TRAIN_COMPUTE)

MACHINE_TYPE = "n1-standard"

VCPU = "4"
DEPLOY_COMPUTE = MACHINE_TYPE + "-" + VCPU
print("Deploy machine type", DEPLOY_COMPUTE)


%%writefile task.py
# Training kmnist using CNN

import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow.python.client import device_lib
import argparse
import os
import sys
tfds.disable_progress_bar()

parser = argparse.ArgumentParser()

parser.add_argument('--epochs', dest='epochs',
                    default=10, type=int,
                    help='Number of epochs.')

args = parser.parse_args()

print('Python Version = {}'.format(sys.version))
print('TensorFlow Version = {}'.format(tf.__version__))
print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))
print('DEVICES', device_lib.list_local_devices())

# Define batch size
BATCH_SIZE = 32

# Load the dataset
datasets, info = tfds.load('kmnist', with_info=True, as_supervised=True)

# Normalize and batch process the dataset
ds_train = datasets['train'].map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)


# Build the Convolutional Neural Network
model = tf.keras.models.Sequential([                               
      tf.keras.layers.Conv2D(16, (3,3), activation=tf.nn.relu, input_shape=(28, 28, 1), padding = "same"),
      tf.keras.layers.MaxPooling2D(2,2),
      tf.keras.layers.Conv2D(16, (3,3), activation=tf.nn.relu, padding = "same"),
      tf.keras.layers.MaxPooling2D(2,2),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(128, activation=tf.nn.relu),
      # TODO: Write the last layer.
      # Hint: KMNIST has 10 output classes.
       tf.keras.layers.Dense(10, activation=tf.nn.softmax)
    ])

model.compile(optimizer = tf.keras.optimizers.Adam(),
      loss = tf.keras.losses.SparseCategoricalCrossentropy(),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

-----------------------------------------------------------------------------------------

Task 4. Train the model

MODEL_DIR = os.getenv("AIP_MODEL_DIR")

model.fit(ds_train, epochs=args.epochs)

# TODO: Save your CNN classifier. 
# Hint: Save it to MODEL_DIR.
model.save(MODEL_DIR)


JOB_NAME = "custom_job_" + TIMESTAMP
MODEL_DIR = "{}/{}".format(BUCKET_NAME, JOB_NAME)

EPOCHS = 5

CMDARGS = [
    "--epochs=" + str(EPOCHS),
]


job = aiplatform.CustomTrainingJob(
    display_name=JOB_NAME,
    requirements=["tensorflow_datasets==4.5.2"],
    # TODO: fill in the remaining arguments for the CustomTrainingJob function.
    script_path="task.py",
    container_uri=TRAIN_IMAGE,
    model_serving_container_image_uri=DEPLOY_IMAGE,
)

MODEL_DISPLAY_NAME = "kmnist-" + TIMESTAMP

# Start the training
try:
    model = job.run(
    model_display_name=MODEL_DISPLAY_NAME,
    replica_count=1,
    accelerator_count=0,
    # TODO: fill in the remaining arguments to run the custom training job function.
    args=CMDARGS,
    machine_type=TRAIN_COMPUTE,
)
except Exception as e:
    # Import here to avoid adding top-level dependency if not needed
    import traceback, sys
    try:
        from google.api_core import exceptions as google_exceptions
    except Exception:
        google_exceptions = None
    err_msg = str(e)
    # Check for PermissionDenied-like message
    if google_exceptions and isinstance(e, google_exceptions.PermissionDenied) or 'Vertex AI API has not been used' in err_msg or 'SERVICE_DISABLED' in err_msg or 'permission denied' in err_msg.lower():
        print('\n=== Vertex AI Permission Error Detected ===\n')
        print('The Vertex AI API appears to be disabled or unavailable for your project.')
        print('Enable it here (replace PROJECT_ID if needed):')
        print('https://console.developers.google.com/apis/api/aiplatform.googleapis.com/overview?project={PROJECT_ID}')
        print('\nAfter enabling, wait a few minutes and retry.\n')
        print('Falling back to local training (if local data/model functions are available)...\n')
        # Attempt to call a local training fallback
        try:
            train_locally  # type: ignore
            train_locally()
        except NameError:
            print('No local training function `train_locally()` found. To train locally, define a `train_locally()` function that accepts your dataset and trains a model, or enable the Vertex AI API and retry.')
        except Exception as le:
            print('Local training attempt failed with error:')
            traceback.print_exc()
    else:
        print('An unexpected error occurred when trying to run the Vertex AI job:')
        traceback.print_exc()


Task 5. Deploy the model to a Vertex Online Prediction Endpoint

DEPLOYED_NAME = "kmnist_deployed-" + TIMESTAMP

TRAFFIC_SPLIT = {"0": 100}

MIN_NODES = 1
MAX_NODES = 1

endpoint = model.deploy(
        deployed_model_display_name=DEPLOYED_NAME,
        accelerator_type=None,
        accelerator_count=0,
        # TODO: subscribe to quicklab
        traffic_split=TRAFFIC_SPLIT,
        machine_type=DEPLOY_COMPUTE,
        min_replica_count=MIN_NODES,
        max_replica_count=MAX_NODES,
    )


-----------------------------------------------------------------------------------

Task 6. Query deployed model on Vertex Online Prediction Endpoint

import tensorflow_datasets as tfds
import numpy as np

tfds.disable_progress_bar()


datasets, info = tfds.load('kmnist', batch_size=-1, with_info=True, as_supervised=True)

test_dataset = datasets['test']


x_test, y_test = tfds.as_numpy(test_dataset)

# subscribe to quicklab
x_test = x_test.astype('float32') / 255.
x_test.shape, y_test.shape


#@title Pick the number of test images
NUM_TEST_IMAGES = 20 #@param {type:"slider", min:1, max:20, step:1}
x_test, y_test = x_test[:NUM_TEST_IMAGES], y_test[:NUM_TEST_IMAGES]


# Import and configure logging
from google.cloud import logging
logging_client = logging.Client()
logger = logging_client.logger('challenge-notebook')


# TODO: use your Endpoint to return prediction for your x_test.

predictions = endpoint.predict(instances=x_test.tolist())


y_predicted = np.argmax(predictions.predictions, axis=1)

correct = sum(y_predicted == np.array(y_test.tolist()))
total = len(y_predicted)

logger.log_text(str(correct/total))

print(
    f"Correct predictions = {correct}, Total predictions = {total}, Accuracy = {correct/total}"
)

-----------------------------------------------------------------------------------------------------

